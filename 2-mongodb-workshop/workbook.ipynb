{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b266ccd",
   "metadata": {},
   "source": [
    "# MongoDB Hands-On Workbook: VCF, bcftools, and MongoDB\n",
    "\n",
    "Use this notebook to follow the interactive exercises for the workshop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30209728",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "At the end of this session, the students will be able to:\n",
    "- **Explain** RBAC model on MongoDB\n",
    "- **Understand** MongoDB CRUD operations:\n",
    "  - Create\n",
    "  - Read\n",
    "  - Update\n",
    "  - Delete\n",
    "- **Explore** the profvided dataset schema with python\n",
    "- **Perform** queries to the provided dataset with python\n",
    "- **Explain** how aggregation works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76104762",
   "metadata": {},
   "source": [
    "## 0) Setup: dependencies and imports\n",
    "\n",
    "This notebook requires a few Python packages for working with MongoDB and VCF files. In the GitHub Codespace used for the workshop these are preinstalled.\n",
    "\n",
    "- `pymongo` — the official MongoDB Python driver (used to connect, run CRUD and aggregation operations).\n",
    "- `vcfpy` — VCF parser/reader for loading variant call format (VCF) files into Python.\n",
    "- `dnspython` — used by some MongoDB SRV connection strings to resolve DNS SRV/TXT records.\n",
    "- `pathlib` / standard library — used for file path handling.\n",
    "\n",
    "The next code cell performs simple imports and prints the Python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e42725e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.5 (main, Jun 11 2025, 15:36:57) [Clang 17.0.0 (clang-1700.0.13.3)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print('Python:', sys.version)\n",
    "# Dependencies are provided by the GitHub Codespace environment\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "import os, vcfpy\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae5f01",
   "metadata": {},
   "source": [
    "# Load .env if it exists and read MONGO_URI (don't echo secret).\n",
    "\n",
    "- Obtain your connection string from the [portal](abds.hpcs.stjude.org)(SSO). What you will get:\n",
    "  - Username\n",
    "  - Password\n",
    "  - Authentication Database\n",
    "  - **Connection String**: All of above combined, along with the server address\n",
    "- Set it as the environment variable `MONGO_URI` or create an env file, or paste interactively\n",
    "\n",
    "```\n",
    "# .env\n",
    "MONGO_URI='mongodb+srv://xxxxxxxxxxxxxx'\n",
    "```\n",
    "\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    autonumber\n",
    "    participant App as Python Client (PyMongo)\n",
    "    participant DNS as DNS (SRV + TXT)\n",
    "    participant LB as Atlas Cluster Endpoint\n",
    "    participant Pri as MongoDB Primary\n",
    "    participant Sec as MongoDB Secondary\n",
    "\n",
    "    App->>DNS: Lookup _mongodb._tcp.<cluster>.mongodb.net\n",
    "    DNS-->>App: SRV hosts/ports + TXT options\n",
    "    App->>LB: TLS handshake (SNI + cert verify)\n",
    "    LB-->>App: TLS session established\n",
    "    App->>Pri: Authenticate (SCRAM-SHA-256)\n",
    "    Pri-->>App: Auth OK (serverHandshake)\n",
    "    App->>Pri: CRUD (find/insert/update/delete)\n",
    "    Pri-->>App: Cursor/results\n",
    "    Note over Pri,Sec: Replication to secondaries (oplog)\n",
    "    App->>Sec: Reads (if readPreference allows)\n",
    "    Sec-->>App: Cursor/results\n",
    "    App-->>LB: Heartbeats & topology discovery (hello)\n",
    "    Note over App: Connection pool manages sockets, retries, timeouts\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6563c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases: ['public', 'zziang_db']\n"
     ]
    }
   ],
   "source": [
    "# load .env if exists\n",
    "load_dotenv()\n",
    "MONGO_URI = os.getenv('MONGO_URI') or input(\"Please set MONGO_URI to your connection string:\")\n",
    "assert MONGO_URI and MONGO_URI.startswith('mongodb'), 'Please check your MONGO_URI format'\n",
    "client = MongoClient(MONGO_URI)\n",
    "db_names = client.list_database_names()\n",
    "print('Databases:', db_names)\n",
    "assert 'public' in db_names, 'Expected to see a public database.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "772dc1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'read', 'db': 'public'}, {'role': 'dbAdmin', 'db': 'zziang_db'}, {'role': 'readWrite', 'db': 'zziang_db'}]\n"
     ]
    }
   ],
   "source": [
    "# Your account were given rule:\n",
    "#    personalDbName = {name}_db\n",
    "#    roles = [\n",
    "#      { role: 'read', db: 'public' },\n",
    "#      { role: 'dbAdmin', db: personalDbName },\n",
    "#      { role: 'readWrite', db: personalDbName }\n",
    "#    ]\n",
    "admin_db = client['admin']\n",
    "status = admin_db.command('connectionStatus')\n",
    "roles = status['authInfo']['authenticatedUserRoles']\n",
    "print(roles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb2e6d6",
   "metadata": {},
   "source": [
    "Check what MongoDB roles mean [here](https://www.mongodb.com/docs/manual/reference/built-in-roles/)\n",
    "\n",
    "### MongoDB data model quick reminder\n",
    "\n",
    "MongoDB organizes data in a simple hierarchy:\n",
    "\n",
    "- Database (DB): a logical namespace for collections. In this workshop you will see `public` (shared read-only data) and your personal database (e.g., `user_db`).\n",
    "  - Collection: a grouping of related documents (similar to a SQL table). Example: `variants` holds VCF-derived variant documents. You can access a collection with `db['variants']`.\n",
    "    - Document: a JSON-like object stored in a collection (BSON). Example document fields from the VCF ingestion: `{'chrom': '1', 'pos': 12345, 'ref': 'A', 'alt': ['T'], 'qual': 50.0, 'filter': 'PASS', 'info': {...}}`.\n",
    "\n",
    "In code: client = MongoClient(MONGO_URI)  ->  db = client['<db_name>']  ->  coll = db['<collection>']  ->  coll.find_one()\n",
    "\n",
    "## Exercise 1 — Explore the `public` dataset (read-only)\n",
    "\n",
    "- List collections\n",
    "- Inspect a sample document, counts, indexes\n",
    "- Distinct values for a field (e.g., `filter`)\n",
    "- Try a forbidden operation to observe RBAC behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aef4913e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections in public: ['reference', 'example']\n",
      "reference Collection got 0 documents\n",
      "example Collection got 2 documents\n",
      "example Collection got 2 documents\n"
     ]
    }
   ],
   "source": [
    "# get the public database\n",
    "db_public = client['public']\n",
    "names = db_public.list_collection_names()\n",
    "print('Collections in public:', names)\n",
    "\n",
    "# list number of documents at each collection\n",
    "for name in names:\n",
    "    col = db_public[name]\n",
    "    num = col.count_documents({})\n",
    "    print(f\"{name} Collection got {num} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5324da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected RBAC error: user is not allowed to do action [remove] on [public.example], full error: {'ok': 0, 'errmsg': 'user is not allowed to do action [remove] on [public.example]', 'code': 8000, 'codeName': 'AtlasError'}\n"
     ]
    }
   ],
   "source": [
    "# RBAC demo: this should fail on public (read-only)\n",
    "coll = db_public['example']\n",
    "if coll is not None:\n",
    "    try:\n",
    "        coll.delete_one({})\n",
    "        print('Unexpected: delete succeeded (RBAC misconfigured?)')\n",
    "    except Exception as e:\n",
    "        print('Expected RBAC error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab4f51",
   "metadata": {},
   "source": [
    "## Exercise 2 — Parse `sample.vcf` and ingest into your personal DB\n",
    "\n",
    "- Choose your personal DB name (or set `DB_NAME` env)\n",
    "- Parse VCF with vcfpy\n",
    "- Insert into `variants` and create indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64f4f7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded VCF content from database\n",
      "Using personal database: zziang_db\n",
      "Cleared variants collection.\n",
      "Cleared variants collection.\n",
      "Inserted documents (this run): 10\n",
      "Inserted documents (this run): 10\n"
     ]
    }
   ],
   "source": [
    "# load VCF doc from `public.example` collection (adjust collection name if different)\n",
    "col = db_public.get_collection('example')\n",
    "vcf_doc = col.find_one({'_id': 'sample.vcf'})\n",
    "\n",
    "import io\n",
    "reader = None\n",
    "if vcf_doc:\n",
    "    vcf_content = vcf_doc.get('content')\n",
    "    if isinstance(vcf_content, (bytes, bytearray)):\n",
    "        vcf_content = vcf_content.decode('utf-8')\n",
    "    # create a vcfpy Reader from the in-memory string\n",
    "    reader = vcfpy.Reader(io.StringIO(vcf_content))\n",
    "    print('Loaded VCF content from database')\n",
    "else:\n",
    "    # fallback: try reading a local sample.vcf file path\n",
    "    local_vcf_path = 'course_material/2-mongodb-workshop/sample.vcf'\n",
    "    try:\n",
    "        reader = vcfpy.Reader.from_path(local_vcf_path)\n",
    "        print(f'Loaded VCF from local path: {local_vcf_path}')\n",
    "    except Exception:\n",
    "        print('VCF document not found in database and local fallback failed.')\n",
    "        raise\n",
    "\n",
    "# get personal DB name (safer checks)\n",
    "DBs = client.list_database_names()\n",
    "person_db = [db_name for db_name in DBs if db_name.endswith('_db')]\n",
    "if not person_db:\n",
    "    raise RuntimeError('No personal database found (expected one ending with ). Set DB_NAME env or ensure your account has a personal DB.')\n",
    "if len(person_db) > 1:\n",
    "    print('Multiple personal DBs found; using the first one:', person_db)\n",
    "DB_NAME = person_db[0]\n",
    "print('Using personal database:', DB_NAME)\n",
    "\n",
    "db = client[DB_NAME]\n",
    "variants = db['variants']\n",
    "\n",
    "# Optional: clear previous runs (disabled by default)\n",
    "CLEAR_PREVIOUS = True\n",
    "if CLEAR_PREVIOUS:\n",
    "    variants.delete_many({})\n",
    "    print('Cleared variants collection.')\n",
    "# Parse VCF and prepare docs\n",
    "docs = []\n",
    "for rec in reader:\n",
    "    alts = [str(a.value) for a in rec.ALT]\n",
    "    info = {k: v for k, v in rec.INFO.items()}\n",
    "    doc = {\n",
    "        'chrom': rec.CHROM,\n",
    "        'pos': rec.POS,\n",
    "        'id': rec.ID,\n",
    "        'ref': rec.REF,\n",
    "        'alt': alts,\n",
    "        'qual': float(rec.QUAL) if rec.QUAL is not None else None,\n",
    "        'filter': 'PASS' if rec.FILTER is None or len(rec.FILTER) == 0 else ';'.join(rec.FILTER),\n",
    "        'info': info,\n",
    "    }\n",
    "    docs.append(doc)\n",
    "\n",
    "# Insert with basic error handling\n",
    "if docs:\n",
    "    try:\n",
    "        variants.insert_many(docs, ordered=False)\n",
    "        # ensure needed indexes\n",
    "        variants.create_index([('chrom', ASCENDING), ('pos', ASCENDING)])\n",
    "        variants.create_index([('filter', ASCENDING)])\n",
    "        variants.create_index([('qual', ASCENDING)])\n",
    "        print('Inserted documents (this run):', len(docs))\n",
    "    except Exception as e:\n",
    "        print('Insert failed:', e)\n",
    "        raise\n",
    "else:\n",
    "    print('No documents parsed from VCF; nothing inserted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b708f411",
   "metadata": {},
   "source": [
    "## Operation breakdown\n",
    "\n",
    "Summary of actions performed in the ingestion exercise:\n",
    "\n",
    "- Query a document from the `public` database (e.g., `public.example`) by its `_id`. The `_id` field is unique within a collection and is the recommended way to retrieve a single document.\n",
    "- Parse the VCF file content (which may be stored as a string or bytes in the document) using `vcfpy.Reader` into records that can be transformed into plain Python dicts.\n",
    "- Transform each VCF record into a MongoDB document with fields such as `chrom`, `pos`, `id`, `ref`, `alt`, `qual`, `filter`, and `info` and collect them for insertion.\n",
    "- Insert parsed documents into the `variants` collection using `insert_many()` (consider de-duplication or `update/upsert` if you run this multiple times).\n",
    "- Create indexes to speed lookups and sorts. Example: `variants.create_index([(\"chrom\", ASCENDING), (\"pos\", ASCENDING)])`. Indexing helps queries like range scans or sorted results be efficient.\n",
    "\n",
    "Safety and best practices:\n",
    "- Avoid unconditional destructive operations like `variants.delete_many({})` in shared or persistent databases; require an explicit confirmation or a configuration flag before clearing data.\n",
    "- When inserting many documents, handle duplicate-key or bulk write errors with try/except and consider `ordered=False` to continue on non-fatal errors.\n",
    "- Add indexes after bulk inserts for faster ingestion, or plan accordingly for your write workload and index maintenance costs.\n",
    "\n",
    "Next steps you might run: inspect a sample document (`variants.find_one()`), check counts (`variants.count_documents({})`), and run the aggregation examples in Exercise 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c9393cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document with generated _id: 68f7103188ea8e2b294bdf5e\n",
      "Inserted document with explicit _id: my-custom-id\n",
      "DuplicateKeyError caught as expected: E11000 duplicate key error collection: zziang_db.demo_id_demo index: _id_ dup key: { _id: \"my-custom-id\" }, full error: {'index': 0, 'code': 11000, 'errmsg': 'E11000 duplicate key error collection: zziang_db.demo_id_demo index: _id_ dup key: { _id: \"my-custom-id\" }', 'keyPattern': {'_id': 1}, 'keyValue': {'_id': 'my-custom-id'}}\n",
      "Demo finished.\n",
      "DuplicateKeyError caught as expected: E11000 duplicate key error collection: zziang_db.demo_id_demo index: _id_ dup key: { _id: \"my-custom-id\" }, full error: {'index': 0, 'code': 11000, 'errmsg': 'E11000 duplicate key error collection: zziang_db.demo_id_demo index: _id_ dup key: { _id: \"my-custom-id\" }', 'keyPattern': {'_id': 1}, 'keyValue': {'_id': 'my-custom-id'}}\n",
      "Demo finished.\n"
     ]
    }
   ],
   "source": [
    "# Demo: _id uniqueness and DuplicateKeyError\n",
    "from pymongo.errors import DuplicateKeyError\n",
    "# Use a temporary collection for demonstration so we don't affect 'variants'\n",
    "db = client[DB_NAME]\n",
    "demo_coll = db.get_collection('demo_id_demo')\n",
    "# Clean up any previous demo content\n",
    "demo_coll.delete_many({})\n",
    "\n",
    "# 1) Insert a document without _id -> MongoDB will generate an ObjectId\n",
    "doc = {'name': 'example1', 'value': 123}\n",
    "res = demo_coll.insert_one(doc)\n",
    "print('Inserted document with generated _id:', res.inserted_id)\n",
    "\n",
    "# 2) Insert a document with an explicit _id\n",
    "explicit = {'_id': 'my-custom-id', 'name': 'example2'}\n",
    "demo_coll.insert_one(explicit)\n",
    "print('Inserted document with explicit _id: my-custom-id')\n",
    "\n",
    "# 3) Attempt to insert another document with the same _id -> DuplicateKeyError expected\n",
    "try:\n",
    "    demo_coll.insert_one({'_id': 'my-custom-id', 'name': 'example2-dup'})\n",
    "except DuplicateKeyError as e:\n",
    "    print('DuplicateKeyError caught as expected:', e)\n",
    "\n",
    "# Cleanup demo collection (optional)\n",
    "demo_coll.delete_many({})\n",
    "print('Demo finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eaa6eb",
   "metadata": {},
   "source": [
    "## Exercise 3 — Update operations\n",
    "\n",
    "- Add `is_high_quality = (qual >= 30 and filter == \"PASS\")`\n",
    "- Bin allele frequency (AF) into bands based on `info.AF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec249496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 10 Modified: 10\n",
      "Matched: 10 Modified: 10\n"
     ]
    }
   ],
   "source": [
    "# 1) Quality flag\n",
    "res1 = variants.update_many(\n",
    "    {'qual': {'$ne': None}},\n",
    "    [{\n",
    "        '$set': {\n",
    "            'is_high_quality': {\n",
    "                '$and': [\n",
    "                    {'$gte': ['$qual', 30]},\n",
    "                    {'$eq': ['$filter', 'PASS']}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }]\n",
    ")\n",
    "print('Matched:', res1.matched_count, 'Modified:', res1.modified_count)\n",
    "\n",
    "# 2) AF bin using $switch and first AF value (Number=A)\n",
    "res2 = variants.update_many(\n",
    "    {'info.AF': {'$exists': True}},\n",
    "    [{\n",
    "        '$set': {\n",
    "            'af_bin': {\n",
    "                '$switch': {\n",
    "                    'branches': [\n",
    "                        {'case': {'$lt': [{'$first': '$info.AF'}, 0.01]}, 'then': '<1%'},\n",
    "                        {'case': {'$lt': [{'$first': '$info.AF'}, 0.05]}, 'then': '1-5%'},\n",
    "                        {'case': {'$lt': [{'$first': '$info.AF'}, 0.5]},  'then': '5-50%'}\n",
    "                    ],\n",
    "                    'default': '>=50%'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }]\n",
    ")\n",
    "print('Matched:', res2.matched_count, 'Modified:', res2.modified_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c4154",
   "metadata": {},
   "source": [
    "## Exercise 4 — Delete operations\n",
    "\n",
    "- Remove low-quality records (e.g., `qual < 10` or `filter != \"PASS\"`)\n",
    "- Pre-check counts before and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5410c731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would remove: 4\n",
      "Deleted: 4\n",
      "Remaining: 6\n",
      "Remaining: 6\n"
     ]
    }
   ],
   "source": [
    "pred = {'$or': [{'qual': {'$lt': 10}}, {'filter': {'$ne': 'PASS'}}]}\n",
    "to_remove = variants.count_documents(pred)\n",
    "print('Would remove:', to_remove)\n",
    "res = variants.delete_many(pred)\n",
    "print('Deleted:', res.deleted_count)\n",
    "print('Remaining:', variants.estimated_document_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbeb128",
   "metadata": {},
   "source": [
    "## Exercise 5 — Aggregations (≈ bcftools)\n",
    "\n",
    "- Count by chromosome\n",
    "- PASS vs non-PASS\n",
    "- Transition/Transversion ratio for SNPs\n",
    "- QUAL histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f82cd479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': '1', 'n': 4}, {'_id': 'X', 'n': 1}, {'_id': '2', 'n': 1}]\n",
      "[{'_id': 'PASS', 'n': 6}]\n",
      "[{'_id': 'transition', 'n': 4}, {'_id': 'transversion', 'n': 1}]\n",
      "[{'_id': 30, 'count': 1}, {'_id': 50, 'count': 1}, {'_id': 60, 'count': 4}]\n",
      "[{'_id': 'PASS', 'n': 6}]\n",
      "[{'_id': 'transition', 'n': 4}, {'_id': 'transversion', 'n': 1}]\n",
      "[{'_id': 30, 'count': 1}, {'_id': 50, 'count': 1}, {'_id': 60, 'count': 4}]\n"
     ]
    }
   ],
   "source": [
    "# 1) Count by chromosome\n",
    "print(list(variants.aggregate([\n",
    "  {'$group': {'_id': '$chrom', 'n': {'$sum': 1}}},\n",
    "  {'$sort': {'n': -1}}\n",
    "])))\n",
    "\n",
    "# 2) PASS vs non-PASS\n",
    "print(list(variants.aggregate([\n",
    "  {'$group': {'_id': '$filter', 'n': {'$sum': 1}}},\n",
    "  {'$sort': {'n': -1}}\n",
    "])))\n",
    "\n",
    "# 3) Ts/Tv for single-nucleotide substitutions\n",
    "pipeline_tstv = [\n",
    "  {'$match': {'$expr': {'$and': [\n",
    "      {'$eq': [{'$strLenCP': '$ref'}, 1]},\n",
    "      {'$in': [1, {'$map': {'input': '$alt', 'as': 'a', 'in': {'$strLenCP': '$$a'}}}]}\n",
    "  ]}}},\n",
    "  {'$unwind': '$alt'},\n",
    "  {'$project': {\n",
    "      'pair': ['$$ROOT.ref', '$alt']\n",
    "  }},\n",
    "  {'$project': {\n",
    "      'class': {\n",
    "        '$switch': {\n",
    "          'branches': [\n",
    "            {'case': {'$in': ['$pair', [['A','G'],['G','A'],['C','T'],['T','C']]]}, 'then': 'transition'}\n",
    "          ],\n",
    "          'default': 'transversion'\n",
    "        }\n",
    "      }\n",
    "  }},\n",
    "  {'$group': {'_id': '$class', 'n': {'$sum': 1}}}\n",
    "]\n",
    "print(list(variants.aggregate(pipeline_tstv)))\n",
    "\n",
    "# 4) QUAL histogram\n",
    "bins = [0,10,20,30,40,50,60,1000]\n",
    "pipeline_hist = [\n",
    "  {'$match': {'qual': {'$ne': None}}},\n",
    "  {'$bucket': {\n",
    "    'groupBy': '$qual',\n",
    "    'boundaries': bins,\n",
    "    'default': '>=1000',\n",
    "    'output': {'count': {'$sum': 1}}\n",
    "  }}\n",
    "]\n",
    "print(list(variants.aggregate(pipeline_hist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02edf32a",
   "metadata": {},
   "source": [
    "### (Optional) Save aggregation results to a `variant_stats` collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee03043e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats docs: 3\n"
     ]
    }
   ],
   "source": [
    "stats_coll = db['variant_stats']\n",
    "stats_coll.delete_many({})\n",
    "stats_coll.insert_many(list(variants.aggregate(pipeline_hist)))\n",
    "print('Stats docs:', stats_coll.estimated_document_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59360fb",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "1. Query VCF slices from a public database. Process the data and insert the result to the personal database. (2%)\n",
    "2. Query the database and get statistics like chromosome count, QUAL distribution, and ALT allele count distribution (3%)\n",
    "\n",
    "Use the starter code cells below as a template — adapt the region(s) and any processing rules as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "479abc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 6 documents for region {'chrom': '1', 'start': 10000, 'end': 12000}\n",
      "Inserted slice documents into personal variants collection\n"
     ]
    }
   ],
   "source": [
    "# Assignment 1 starter: Query VCF slices from public DB and insert processed docs into personal `variants` collection\n",
    "# Adjust `region` to the slice you want (chrom, start, end)\n",
    "region = {'chrom': '1', 'start': 10000, 'end': 12000}  # example: chr1:10000-12000\n",
    "\n",
    "# Read VCF content from public.example (same approach used earlier)\n",
    "col_example = db_public.get_collection('example')\n",
    "vcf_doc = col_example.find_one({'_id': 'sample.vcf'})\n",
    "import io\n",
    "if not vcf_doc:\n",
    "    raise RuntimeError('sample.vcf not found in public.example collection')\n",
    "vcf_content = vcf_doc.get('content')\n",
    "if isinstance(vcf_content, (bytes, bytearray)):\n",
    "    vcf_content = vcf_content.decode('utf-8')\n",
    "reader = vcfpy.Reader(io.StringIO(vcf_content))\n",
    "\n",
    "# filter records by region and prepare docs\n",
    "slice_docs = []\n",
    "for rec in reader:\n",
    "    try:\n",
    "        pos = rec.POS\n",
    "        chrom = rec.CHROM\n",
    "        if chrom == region['chrom'] and region['start'] <= pos <= region['end']:\n",
    "            alts = [str(a.value) for a in rec.ALT]\n",
    "            info = {k: v for k, v in rec.INFO.items()}\n",
    "            doc = {\n",
    "                'chrom': chrom,\n",
    "                'pos': pos,\n",
    "                'id': rec.ID,\n",
    "                'ref': rec.REF,\n",
    "                'alt': alts,\n",
    "                'qual': float(rec.QUAL) if rec.QUAL is not None else None,\n",
    "                'filter': 'PASS' if rec.FILTER is None or len(rec.FILTER) == 0 else ';'.join(rec.FILTER),\n",
    "                'info': info,\n",
    "            }\n",
    "            slice_docs.append(doc)\n",
    "    except Exception as e:\n",
    "        print('Skipping record due to parse error:', e)\n",
    "\n",
    "# Insert into personal variants collection (db and variants assumed defined earlier)\n",
    "if slice_docs:\n",
    "    print('Prepared', len(slice_docs), 'documents for region', region)\n",
    "    # optional: do not overwrite existing docs with same chrom/pos - you can de-duplicate or upsert\n",
    "    try:\n",
    "        variants.insert_many(slice_docs, ordered=False)\n",
    "        print('Inserted slice documents into personal variants collection')\n",
    "    except Exception as e:\n",
    "        print('Insert failed:', e)\n",
    "else:\n",
    "    print('No records found for region', region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5774eb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chromosome counts:\n",
      "[{'_id': '1', 'count': 10}, {'_id': '2', 'count': 1}, {'_id': 'X', 'count': 1}]\n",
      "QUAL histogram:\n",
      "[{'_id': 0, 'count': 1}, {'_id': 20, 'count': 1}, {'_id': 30, 'count': 1}, {'_id': 50, 'count': 9}]\n",
      "ALT allele count distribution:\n",
      "[{'_id': 0, 'count': 1}, {'_id': 20, 'count': 1}, {'_id': 30, 'count': 1}, {'_id': 50, 'count': 9}]\n",
      "ALT allele count distribution:\n",
      "[{'_id': 1, 'count': 10}, {'_id': 2, 'count': 2}]\n",
      "[{'_id': 1, 'count': 10}, {'_id': 2, 'count': 2}]\n"
     ]
    }
   ],
   "source": [
    "# Assignment 2 starter: compute statistics from the personal `variants` collection\n",
    "# 1) Chromosome counts\n",
    "print('Chromosome counts:')\n",
    "print(list(variants.aggregate([\n",
    "    {'$group': {'_id': '$chrom', 'count': {'$sum': 1}}},\n",
    "    {'$sort': {'count': -1}}\n",
    "])) )\n",
    "\n",
    "# 2) QUAL distribution (bucketed)\n",
    "bins = [0,10,20,30,40,50,1000]\n",
    "pipeline_qual = [\n",
    "    {'$match': {'qual': {'$ne': None}}},\n",
    "    {'$bucket': {\n",
    "        'groupBy': '$qual',\n",
    "        'boundaries': bins,\n",
    "        'default': '>=1000',\n",
    "        'output': {'count': {'$sum': 1}}\n",
    "    }}\n",
    "]\n",
    "print('QUAL histogram:')\n",
    "print(list(variants.aggregate(pipeline_qual)))\n",
    "\n",
    "# 3) ALT allele count distribution (how many alt alleles per record)\n",
    "pipeline_altcount = [\n",
    "    {'$project': {'n_alt': {'$size': {'$ifNull': ['$alt', []]}}}}\n",
    "]\n",
    "pipeline_altcount_group = [\n",
    "    {'$group': {'_id': '$n_alt', 'count': {'$sum': 1}}},\n",
    "    {'$sort': {'_id': 1}}\n",
    "]\n",
    "print('ALT allele count distribution:')\n",
    "alt_counts = list(variants.aggregate(pipeline_altcount + pipeline_altcount_group))\n",
    "print(alt_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e108c07",
   "metadata": {},
   "source": [
    "## Quick reference: `_id`, indexes, and schema choice\n",
    "\n",
    "- `_id` behavior: If you insert a document without an `_id` field, MongoDB automatically generates a unique ObjectId and assigns it to `_id`. This guarantees uniqueness per collection unless you explicitly provide your own `_id`.\n",
    "- Index benefits: Indexes improve query performance for lookups on the indexed fields and can make sorting efficient when the index matches the sort pattern. They may increase write cost and storage for index data, but do not enforce schema. Creating appropriate indexes is demonstrated earlier when we create indexes on `chrom` and `pos`.\n",
    "- Embedding vs referencing: For unbounded or very large arrays that grow over time (e.g., monthly samples per site), referencing (a separate `samples` collection with a site_id field) is more scalable than embedding all samples inside the parent `site` document. Embedding is a good choice when related subdocuments are bounded and usually retrieved together with the parent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
